{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from load import load_glove_embedding, load_vocabulary, load_tweets\n",
    "from FeaturesBuilder import FeaturesBuilder\n",
    "from neural_net_utils import keras_compile, to_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load glove embedding\n",
    "word_vect = load_glove_embedding('glove_embeddings.npy')\n",
    "\n",
    "# Load vocabulary\n",
    "vocab = load_vocabulary('vocab.pkl')\n",
    "\n",
    "# Load tweets\n",
    "tweets_df = load_tweets('../twitter-datasets', full=True) \n",
    "\n",
    "# Define features builder instance\n",
    "SEQ_LENGTH = 50\n",
    "features_builder = FeaturesBuilder(tweets_df, vocab, word_vect, target_length=SEQ_LENGTH)\n",
    "# ___ available methods ___\n",
    "# build_avg_tweet_embedding\n",
    "# build_word_embedding_sequences\n",
    "# build_vocab_idx_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic ML on GloVe tweet embedding average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the word embeddings are averaged over each tweet to build features with (embedding_dim = 20) shape. We try with the following methods to fit a classifier over that space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = features_builder.build_avg_tweet_embedding()\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "\n",
    "def score(model):\n",
    "    y_pred = model.predict(x_test) > 0.5\n",
    "    print('accuracy:', accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(x_train, y_train)\n",
    "score(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We realise that a linear separation of the GloVe embedding space provides a baseline accuracy of 60%, we will compare this baseline to the next result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=120)\n",
    "model.fit(x_train, y_train)\n",
    "score(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random forest classifier better captures features that are not correlated linearly and thus improves the score significantly over linear regression.\n",
    "We are no learning a non linear sepration of the 20 dimensional space where tweets live."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to learn a continuous separation using a densely connected perceptron. Dropout is used to reduce overfitting and thus better generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(50),\n",
    "        \n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(100),\n",
    "        \n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(50),\n",
    "        \n",
    "        layers.Dropout(0.1),\n",
    "\n",
    "        layers.Dense(1, activation=\"sigmoid\", name=\"out\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "keras_compile(model)\n",
    "\n",
    "#print(model.summary())\n",
    "\n",
    "# train\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=64,\n",
    "    epochs=10,\n",
    "    validation_data=(x_test, y_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying various network architectures, altering width and depth, we found that none of them was able to outperform the linear classifier. This probably means that the embedding space has to be separated using high order functions, which makes it more difficult for the perceptron to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on glove with word sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we build sequences of word embeddings. Those sequences are front padded to provide a fixed tensor size to the neural network. Features now have a shape of (sequence_length = 50, embedding_dim = 20). We gain the information of words order and provide a pre-trained representation of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dataset\n",
    "x, y = features_builder.build_word_embedding_sequences()\n",
    "x_train, x_test, y_train, y_test = to_tensor(*train_test_split(x, y, test_size=0.33, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Bidirectional(layers.LSTM(100, dropout=0.4, return_sequences=True)),\n",
    "        layers.Bidirectional(layers.LSTM(200, dropout=0.4, return_sequences=True)),\n",
    "        layers.Bidirectional(layers.LSTM(100, dropout=0.4,)),\n",
    "\n",
    "        layers.Dense(1, activation=\"sigmoid\", name=\"out\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "keras_compile(model)\n",
    "\n",
    "#print(model.summary())\n",
    "\n",
    "# train\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=64,\n",
    "    epochs=8,\n",
    "    validation_data=(x_test, y_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We switch to a trainable embedding, where word representations are learned in respect to the classification task and not from co occurences as in GloVe. \n",
    "\n",
    "With each tweet, backpropagation from the last layers will update the weights associated with each word.\n",
    "\n",
    "Here we pass a sequence of word index in the vocabulary, of shape (sequence_length = 50).\n",
    "\n",
    "This means that the embedding layer is not provided with pre-defined meaning but will  infer it from the set of indexes we pass as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dataset\n",
    "x, y = features_builder.build_vocab_idx_sequences()\n",
    "x_train, x_test, y_train, y_test = to_tensor(*train_test_split(x, y, test_size=0.33, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Embedding(len(vocab)+1, 100),\n",
    "        \n",
    "        layers.Bidirectional(layers.LSTM(100, dropout=0.4, return_sequences=True)),\n",
    "        layers.Bidirectional(layers.LSTM(200, dropout=0.4, return_sequences=True)),\n",
    "        layers.Bidirectional(layers.LSTM(100, dropout=0.4, return_sequences=True)),\n",
    "        layers.Bidirectional(layers.LSTM(50, dropout=0.4)),\n",
    "        \n",
    "        layers.Dense(1, activation=\"sigmoid\", name=\"out\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "keras_compile(model)\n",
    "\n",
    "#print(model.summary())\n",
    "\n",
    "# train\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=64,\n",
    "    epochs=8,\n",
    "    validation_data=(x_test, y_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping the indexes sequence used to train an LSTM network, we now train a convolutional network. Five parallel channels with convolutional layers of various kernel sizes are concatenated and fed to two densely connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dataset\n",
    "x, y = features_builder.build_vocab_idx_sequences()\n",
    "x_train, x_test, y_train, y_test = to_tensor(*train_test_split(x, y, test_size=0.5, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "embedding_dim = 100\n",
    "seq_len = SEQ_LENGTH\n",
    "\n",
    "sequence_input = keras.Input(shape=(seq_len,), dtype='int32')\n",
    "\n",
    "embedding_layer = layers.Embedding(len(vocab)+1,\n",
    "                            embedding_dim,\n",
    "                            input_length=seq_len,\n",
    "                            trainable=True)\n",
    "\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "convs = []\n",
    "filter_sizes = [2,3,4,5,6]\n",
    "for filter_size in filter_sizes:\n",
    "    l_conv = layers.Conv1D(filters=200,\n",
    "                            #padding=\"same\",\n",
    "                           input_shape=(seq_len, embedding_dim),\n",
    "                    kernel_size=filter_size, \n",
    "                    activation='relu')(embedded_sequences)\n",
    "    l_pool = layers.MaxPooling1D(filter_size)(l_conv)\n",
    "    l_conv = layers.Conv1D(filters=100,\n",
    "                           #padding=\"same\",\n",
    "                            input_shape=(seq_len, embedding_dim),\n",
    "                            kernel_size=filter_size, \n",
    "                            activation='relu')(l_pool)\n",
    "    l_pool = layers.GlobalMaxPooling1D()(l_conv)\n",
    "    convs.append(l_pool)\n",
    "l_merge = layers.Concatenate(axis=1)(convs)\n",
    "\n",
    "# Dense layers\n",
    "x = layers.Dropout(0.1)(l_merge)  \n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.Dropout(0.4)(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "preds = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "\n",
    "keras_compile(model)\n",
    "\n",
    "#print(model.summary())\n",
    "\n",
    "# train\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=64,\n",
    "    epochs=5,\n",
    "    validation_data=(x_test, y_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores obtained with the CNN architectures are very close to the ones attained with the LSTM network. We find that these two approaches while different in nature are able to learn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
