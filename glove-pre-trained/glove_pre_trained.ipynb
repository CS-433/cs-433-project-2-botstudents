{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following Notebook implements classification using a convolutional neural network with GloVe pre-trained embeddings.\n",
    "Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To download the pre-trained GloVe embeddings please refer to the following link : \n",
    "\n",
    "https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "###### glove.twitter.27B.25d.txt should be located at the same root as this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow :  \n",
    "Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo,\n",
    "Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis,\n",
    "Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow,\n",
    "Andrew Harp, Geoffrey Irving, Michael Isard, Rafal Jozefowicz, Yangqing Jia,\n",
    "Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mané, Mike Schuster,\n",
    "Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Jonathon Shlens,\n",
    "Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker,\n",
    "Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas,\n",
    "Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke,\n",
    "Yuan Yu, and Xiaoqiang Zheng.\n",
    "TensorFlow: Large-scale machine learning on heterogeneous systems, 2015.  \n",
    "Software available from tensorflow.org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pXPQxDgbclp2",
    "outputId": "05521433-6401-4c72-a83f-9dc905a14c06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "print(tf.__version__)\n",
    "\n",
    "from load_utils import load_glove_embedding, load_vocabulary, load_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has beeen run using tensorflow version 2.3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modified preprocessing function from Stanford, specific for Glove on Tweets\n",
    "import stanford_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TVvy7QyGiJSN",
    "outputId": "69e94886-7e28-48c1-c78a-6e47fbd21de4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 2500000 tweets in dataframe with columns: Index(['text', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "tweets = load_tweets(full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "g9N2acVXkWVq"
   },
   "outputs": [],
   "source": [
    "#apply preprocessing for GloVe\n",
    "tweets['tokenized'] = tweets.text.apply(stanford_preprocessing.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NY1P5scskkRb"
   },
   "outputs": [],
   "source": [
    "x = tweets['tokenized']\n",
    "y = tweets.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "P6K4XKf5kl33"
   },
   "outputs": [],
   "source": [
    "#max_features is max vocabulary length, chosen for reasonable memory cost \n",
    "max_features=50000\n",
    "#max_test_length is max number of words per tweet\n",
    "max_text_length=40\n",
    "\n",
    "#using keras tokenizer to extract words\n",
    "x_tokenizer=text.Tokenizer(max_features)\n",
    "x_tokenizer.fit_on_texts(list(x))\n",
    "x_tokenized=x_tokenizer.texts_to_sequences(x)\n",
    "\n",
    "#zero padding for sequences with less words than max_text_length\n",
    "x_train_val=sequence.pad_sequences(x_tokenized,maxlen=max_text_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i_-DRzIckned",
    "outputId": "8b1befb5-ee28-4796-e08c-2230dd6e0d2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193514 word vectors\n"
     ]
    }
   ],
   "source": [
    "#import pre-trained twitter glove embeddings with 25 featurers\n",
    "embedding_dim=25\n",
    "embedding_index=dict()\n",
    "f=open(\"glove.twitter.27B.25d.txt\")\n",
    "for line in f:\n",
    "    values=line.split()\n",
    "    word=values[0]\n",
    "    coefs=np.asarray(values[1:],dtype='float32') \n",
    "    embedding_index[word]=coefs\n",
    "    \n",
    "f.close()\n",
    "print(f'Found {len(embedding_index)} word vectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "WPjQaeWGkxzL"
   },
   "outputs": [],
   "source": [
    "#create embeddings matrix for Keras Embedding Layer\n",
    "embedding_matrix=np.zeros((max_features,embedding_dim))\n",
    "for word,index in x_tokenizer.word_index.items():\n",
    "    if index>max_features-1:\n",
    "        break\n",
    "    else:\n",
    "        embedding_vector=embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index]=embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The fairly generic neural network architecture below could have been optimized and therefore the resulting accuracy could have been improved but we have put our energy on improving other methods. This notebook highlights that retraining GloVe embeddings using supervised learning yields better results if little time is spent in searching for hyperparameters and tailoring a neural network architecture. \n",
    "##### It also highlights that reaching a good accuracy is much easier with pre-trained embeddings then with our own (particularly when they've been pre-trained on a similar dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training using pre-trained GloVe embeddings : Best Accuracy on Validation set is 82.47 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "SFTY_ueBk_SI"
   },
   "outputs": [],
   "source": [
    "#build Sequential model, first layer is Embedding Layer (not trainable because already trained with unsupervised learning)\n",
    "model=Sequential()\n",
    "model.add(Embedding(max_features,\n",
    "                   embedding_dim,\n",
    "                   embeddings_initializer=tf.keras.initializers.Constant(\n",
    "                   embedding_matrix),\n",
    "                   trainable=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fWv24CiSk_6b",
    "outputId": "aadcb846-b5f8-4eb2-e8bc-68bdf5e34a84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 25)          1250000   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 250)         19000     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, None, 250)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 250)         312750    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 1,644,751\n",
      "Trainable params: 394,751\n",
      "Non-trainable params: 1,250,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#ConvNet Architecture for Classification on word Embeddings\n",
    "filters=250\n",
    "kernel_size=3\n",
    "hidden_dims=250\n",
    "\n",
    "model.add(Conv1D(filters,\n",
    "                kernel_size,\n",
    "                padding='valid'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Conv1D(filters,\n",
    "                5,\n",
    "                padding='valid',\n",
    "                activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(hidden_dims,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "t3dRcZ6xtTIk"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "tB7vzW34lHNf"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_train_val, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hi9OUxeTtf0r",
    "outputId": "8b975190-975a-4b3a-eb4a-370eb1eb872c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "13086/13086 [==============================] - 428s 33ms/step - loss: 0.4268 - accuracy: 0.7903 - val_loss: 0.4063 - val_accuracy: 0.7988\n",
      "Epoch 2/15\n",
      "13086/13086 [==============================] - 460s 35ms/step - loss: 0.3921 - accuracy: 0.8139 - val_loss: 0.3897 - val_accuracy: 0.8154\n",
      "Epoch 3/15\n",
      "13086/13086 [==============================] - 457s 35ms/step - loss: 0.3762 - accuracy: 0.8231 - val_loss: 0.3842 - val_accuracy: 0.8189\n",
      "Epoch 4/15\n",
      "13086/13086 [==============================] - 449s 34ms/step - loss: 0.3650 - accuracy: 0.8299 - val_loss: 0.3804 - val_accuracy: 0.8218\n",
      "Epoch 5/15\n",
      "13086/13086 [==============================] - 462s 35ms/step - loss: 0.3561 - accuracy: 0.8349 - val_loss: 0.3764 - val_accuracy: 0.8226\n",
      "Epoch 6/15\n",
      "13086/13086 [==============================] - 464s 35ms/step - loss: 0.3478 - accuracy: 0.8391 - val_loss: 0.3810 - val_accuracy: 0.8233\n",
      "Epoch 7/15\n",
      "13086/13086 [==============================] - 451s 34ms/step - loss: 0.3403 - accuracy: 0.8428 - val_loss: 0.3815 - val_accuracy: 0.8234\n",
      "Epoch 8/15\n",
      "13086/13086 [==============================] - 473s 36ms/step - loss: 0.3338 - accuracy: 0.8463 - val_loss: 0.3863 - val_accuracy: 0.8228\n",
      "Epoch 9/15\n",
      "13086/13086 [==============================] - 480s 37ms/step - loss: 0.3278 - accuracy: 0.8496 - val_loss: 0.3833 - val_accuracy: 0.8247\n",
      "Epoch 10/15\n",
      "13086/13086 [==============================] - 468s 36ms/step - loss: 0.3222 - accuracy: 0.8522 - val_loss: 0.3888 - val_accuracy: 0.8245\n",
      "Epoch 11/15\n",
      "13086/13086 [==============================] - 477s 36ms/step - loss: 0.3168 - accuracy: 0.8550 - val_loss: 0.3929 - val_accuracy: 0.8229\n",
      "Epoch 12/15\n",
      "13086/13086 [==============================] - 441s 34ms/step - loss: 0.3120 - accuracy: 0.8572 - val_loss: 0.3972 - val_accuracy: 0.8210\n",
      "Epoch 13/15\n",
      "13086/13086 [==============================] - 462s 35ms/step - loss: 0.3075 - accuracy: 0.8595 - val_loss: 0.3951 - val_accuracy: 0.8225\n",
      "Epoch 14/15\n",
      "13086/13086 [==============================] - 460s 35ms/step - loss: 0.3032 - accuracy: 0.8615 - val_loss: 0.4124 - val_accuracy: 0.8206\n",
      "Epoch 15/15\n",
      "13086/13086 [==============================] - 518s 40ms/step - loss: 0.2992 - accuracy: 0.8631 - val_loss: 0.4214 - val_accuracy: 0.8223\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ae03f7090>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the following batch size and epoch number are close to optimal,\n",
    "#they have been found by manually testing different values\n",
    "\n",
    "batch_size=128\n",
    "epochs=15\n",
    "model.fit(X_train,y_train,\n",
    "         batch_size=batch_size,\n",
    "         epochs=epochs,\n",
    "         validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCoHO4v3u9sd"
   },
   "source": [
    "#### Retraining pre-trained GloVe embeddings : Best Accuracy on Validation set is 85.76 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "lM6_XzkRu9sd"
   },
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(max_features,\n",
    "                   embedding_dim,\n",
    "                   embeddings_initializer=tf.keras.initializers.Constant(\n",
    "                   embedding_matrix),\n",
    "                   trainable=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08ArqFN1u9sd",
    "outputId": "ae6d9c4d-3ab5-4d45-8c20-b2bd2ebc6bde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 25)          1250000   \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, None, 250)         19000     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, None, 250)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, None, 250)         312750    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 1,644,751\n",
      "Trainable params: 1,644,751\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "filters=250\n",
    "kernel_size=3\n",
    "hidden_dims=250\n",
    "\n",
    "model.add(Conv1D(filters,\n",
    "                kernel_size,\n",
    "                padding='valid'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Conv1D(filters,\n",
    "                5,\n",
    "                padding='valid',\n",
    "                activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(hidden_dims,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "45brt8Mzu9se"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3_zPX1Alu9se",
    "outputId": "3e5218e3-c012-4e93-a790-68f2c085274d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "13086/13086 [==============================] - 698s 53ms/step - loss: 0.3653 - accuracy: 0.8313 - val_loss: 0.3346 - val_accuracy: 0.8485\n",
      "Epoch 2/6\n",
      "13086/13086 [==============================] - 636s 49ms/step - loss: 0.3244 - accuracy: 0.8551 - val_loss: 0.3280 - val_accuracy: 0.8536\n",
      "Epoch 3/6\n",
      "13086/13086 [==============================] - 647s 49ms/step - loss: 0.3063 - accuracy: 0.8648 - val_loss: 0.3201 - val_accuracy: 0.8563\n",
      "Epoch 4/6\n",
      "13086/13086 [==============================] - 689s 53ms/step - loss: 0.2916 - accuracy: 0.8723 - val_loss: 0.3203 - val_accuracy: 0.8576\n",
      "Epoch 5/6\n",
      "13086/13086 [==============================] - 845s 65ms/step - loss: 0.2782 - accuracy: 0.8788 - val_loss: 0.3249 - val_accuracy: 0.8572\n",
      "Epoch 6/6\n",
      "13086/13086 [==============================] - 880s 67ms/step - loss: 0.2648 - accuracy: 0.8850 - val_loss: 0.3286 - val_accuracy: 0.8562\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ab9bdc710>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=128\n",
    "epochs=6\n",
    "model.fit(X_train,y_train,\n",
    "         batch_size=batch_size,\n",
    "         epochs=epochs,\n",
    "         validation_data=(X_test,y_test))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "glove_pre-trained_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
