{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe Embedding\n",
    "\n",
    "In this notebook we will follow different steps in order to create different embeddings for different processing scenarios. \\\n",
    "We will be storing data as text files and pickle files, before anything else the following empty folders needs to be created : \n",
    "- embeddings \n",
    "- vocab \n",
    "- pptweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all necessary librairies \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from scipy.sparse import *\n",
    "import random\n",
    "\n",
    "from pp_utils import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating base vocabulary\n",
    "\n",
    "To generate the vocabulary we use the following lines in the terminal at current folder location and in the following order : \n",
    "- cat  twitter-datasets/train_pos_full.txt twitter-datasets/train_neg_full.txt | sed \"s/ /\\n/g\" | grep -v \"^\\s*$\" | sort | uniq -c > vocab/built_voc_pp0.txt\n",
    "- cat vocab/built_voc_pp0.txt | sed \"s/^\\s\\+//g\" | sort -rn | grep -v \"^[1234]\\s\" | cut -d' ' -f2 > vocab/vocab_cut_pp0.txt\n",
    "\n",
    "This can be achieved by double clickings on the shell files in the folder in the following order :\n",
    "- **build_vocab.sh**\n",
    "- **cut_vocab.sh** \n",
    "\n",
    "Now that we have generated a vocabulary from the tweets, we can use the cooc and glove functions located in the pp_utils.py file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 200000 tweets in dataframe with columns: Index(['text', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Relocating the tweets txt files in the right format and folder to be used later\n",
    "\n",
    "tweets = load_tweets()\n",
    "with open(\"pptweets/tweets_pp0.txt\", \"w\", encoding = 'utf8') as txt_file:\n",
    "    for tweet in np.array(tweets['text']) :\n",
    "        txt_file.write(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating cooccurrence matrix ...\n",
      "loading cooccurrence matrix ...\n",
      "8583351 nonzero entries\n",
      "using nmax = 100 , cooc.max() = 207302\n",
      "initializing embeddings\n",
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n"
     ]
    }
   ],
   "source": [
    "# Create a vocab pickle stored in the vocab folder \n",
    "pickle_vocab(0)\n",
    "\n",
    "# Uses the vocabulary pickle to build embeddings \n",
    "cooc(0)\n",
    "glove(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GloVe word embeddings are stored in the embeddings folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing \n",
    "\n",
    "We implemented three different tasks of preprocessing :\n",
    "- (1) The creation of tokens representing numerical and/or textual patterns such as emoticons, word elongation, numbers, repeating punctuation. \n",
    "- (2)  Hashtag processing both using a \\<hashtag\\> token to quantify the use of  hashtags and splitting hashtags into known words in the vocabulary. \n",
    "- (3) Replacing \\<hashtag\\> by a stopword token. \n",
    "\n",
    "We decided to test four combination of these preprocessing tasks\n",
    "- 0 No preprocessing \n",
    "- 1 Tokenization (1)\n",
    "- 2 Tokenization and Hashtag Split (1) and (2)\n",
    "- 3 Tokenization and Stop Words (1) and (3)\n",
    "\n",
    "##### *We assumed that testing our preprocessing on the reduced dataset would be sufficient to evaluate efficiency.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing preprocessing functions \n",
    "from preprocessing import *\n",
    "\n",
    "# Loading tweets in dataframes \n",
    "tweets_pp1 = create_df(100000, 100000)\n",
    "tweets_pp2 = create_df(100000, 100000)\n",
    "tweets_pp3 = create_df(100000, 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make tokens \n",
    "\n",
    "The following tokens are created : \n",
    "- \\<elong\\> \\<repeat\\> \\<number\\> \n",
    "- \\<heart\\> \\<smiling\\> \\<tongue\\> \\<angrysad\\> \\<skeptical\\> \\<kissing\\> \\<brokenheart\\> \\<surprised\\> \n",
    "\n",
    "The textual patterns for emoticons were found on https://en.wikipedia.org/wiki/List_of_emoticons \\\n",
    "These patterns are stored as text files in the Emoticon folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Preprocessing Option \n",
    "tweets_pp1.text = tweets.text.apply(lambda x: preprocess_tweet(x,  tokenize=True, split_hashtags=False, remove_stopwords=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting words from hashtag\n",
    "\n",
    "Splitting the hashtags is a difficult and uncertain task because there is no way to be absolutely sure that the hashtag will be correctly splitted. \n",
    "\n",
    "In order to get better results, we formed a list of all the words used more than a hundred times in all the reduced dataset. What the splitting function does is that it tries to extract the longest possible words out of the hashtags, using a list of the most used words guarantee that the split don't return garbage. Going further, we decided not to return the split if it only return one or two letters words.\n",
    "\n",
    "The topword list is created thanks to the CountVectorizer and stored in a txt file for later use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create topwords list from tokenized tweets\n",
    "create_topwords(tweets, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Preprocessing Option \n",
    "tweets_pp2.text = tweets_pp1.text.apply(lambda x: preprocess_tweet(x,  tokenize=False, split_hashtags=True, remove_stopwords=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing stopwords \n",
    "\n",
    "We used a stopwords list available at https://www.ranks.nl/stopwords from which we removed all the words expressing a negation in order to limit the loss of meaning. It is located in the PpreprocessingFiles folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third Preprocessing Option \n",
    "tweets_pp3.text = tweets_pp1.text.apply(lambda x: preprocess_tweet(x,  tokenize=False, split_hashtags=False, remove_stopwords=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the preprocessed tweets for later use \n",
    "\n",
    "We save the tweets in a folder we'll use later to train our models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pptweets/tweets_pp1.txt\", \"w\", encoding = 'utf8') as txt_file:\n",
    "    for tweet in np.array(tweets_pp1['text']) :\n",
    "        txt_file.write(tweet)\n",
    "        \n",
    "with open(\"pptweets/tweets_pp2.txt\", \"w\", encoding = 'utf8') as txt_file:\n",
    "    for tweet in np.array(tweets_pp2['text']) :\n",
    "        txt_file.write(tweet)\n",
    "        \n",
    "with open(\"pptweets/tweets_pp3.txt\", \"w\", encoding = 'utf8') as txt_file:\n",
    "    for tweet in np.array(tweets_pp3['text']) :\n",
    "        txt_file.write(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating preprocessing vocabulary \n",
    "\n",
    "Pre-processing introduces new tokens, and as a result, creates new words that are not taken into account in the first vocabulary. This is the reason why we need to conpute our own vocabulary from the preprocessed tweets in order to create new embeddings. We first need to apply preprocessing on the full data set, this operation can take quite a lot of time. We already placed the fully preprocessed tweets in the vocab folder. \n",
    "\n",
    "The preprocessing on the full data set is done by executing the following cell :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING : TAKES 40 min - already computed for convenience \n",
    "# Preprocessing on the full dataset \n",
    "\n",
    "tweets_full = load_tweets(full = True)\n",
    "tweets_pp1_full = create_df(1250000, 1250000)\n",
    "tweets_pp1_full.text = tweets_pp1_full.text.apply(lambda x: preprocess_tweet(x,  tokenize=False, split_hashtags=True, remove_stopwords=False))\n",
    "\n",
    "with open(\"pptweets/tweets_pp1_full.txt\", \"w\", encoding = 'utf8') as txt_file:\n",
    "    for tweet in np.array(tweets_pp1_full['text']) :\n",
    "        txt_file.write(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating a vocabulary from the tokenized tweets using the following shell : \n",
    "- cat  pptweets/tweets_pp1_full | sed \"s/ /\\n/g\" | grep -v \"^\\s*$\" | sort | uniq -c > vocab/built_voc_pp.txt\n",
    "- cat vocab/built_voc_pp.txt | sed \"s/^\\s\\+//g\" | sort -rn | grep -v \"^[1234]\\s\" | cut -d' ' -f2 > vocab/vocab_cut_pp.txt\n",
    "\n",
    "This can be achieved by double clickings on the shell files in the folder in the following order :\n",
    "- **build_vocab_pp.sh**\n",
    "- **cut_vocab_pp.sh**\n",
    "\n",
    "The vocabulary is stored as vocab_cut_pp1.txt in the vocab folder. \\\n",
    "This operation is time consuming as it uses the full dataset, but we don't need to do it all over again for the other datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the txt file into a pickle \n",
    "pickle_vocab(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashtag split \n",
    "\n",
    "The words in the hashtag already existed in the previous vocabulary, for that matter we can re-use the vocabulary we just computed, we just need to add a \\<hashtag\\> token. \n",
    "\n",
    "#### Stopwords \n",
    "\n",
    "Removing stopwords from the tweets makes all stopwords disappear from the vocabulary. On the same basis, we can use the same vocabulary as before adding a \\<stopword\\> token and removing all stopwords from the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a cut vocabulary for the second and third preprocessing options\n",
    "create_vocab_pp2()\n",
    "create_vocab_pp3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create gloVe embeddings \n",
    "\n",
    "Now that we have both the vocabularies and different preprocessed tweet sets we can now compute the word embeddings for each case. \n",
    "The embeddings are saved in the embedding folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings for the unprocessed tweets \n",
    "glove(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating embeddings for the three preprocessing scenarios \n",
    "glove(1)\n",
    "glove(2)\n",
    "glove(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
