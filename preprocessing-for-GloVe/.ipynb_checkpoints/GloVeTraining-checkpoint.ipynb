{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GloVeTraining3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RDfyO30AI8h"
      },
      "source": [
        "# Training Tweets with GloVe Embedding Word Vectorization \n",
        "\n",
        "The GloVe Embeddings and vocabulary were trained thanks to the GloveEmbeddings.ipynb notebook. \\\n",
        "\n",
        "As a reminder : \n",
        "\n",
        "We implemented three different tasks of preprocessing :\n",
        "- (1) The creation of tokens representing numerical and/or textual patterns such as emoticons, word elongation, numbers, repeating punctuation. \n",
        "- (2)  Hashtag processing both using a \\<hashtag\\> token to quantify the use of  hashtags and splitting hashtags into known words in the vocabulary. \n",
        "- (3) Replacing \\<hashtag\\> by a stopword token. \n",
        "\n",
        "We decided to test four combination of these preprocessing tasks\n",
        "- 0 No preprocessing \n",
        "- 1 Tokenization (1)\n",
        "- 2 Tokenization and Hashtag Split (1) and (2)\n",
        "- 3 Tokenization and Stop Words (1) and (3)\n",
        "\n",
        "The embeddings for each preprocessing options are stored in the embeddings folder. \n",
        "The corresponding vocabulary is stored in the vocab folder. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNH0bD-GAI8p"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import pickle as pkl\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from load_utils_pp import *\n",
        "from FeaturesBuilder import FeaturesBuilder"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZx86SovAI8q"
      },
      "source": [
        "## Word Vector Average \n",
        "\n",
        "We started by computing the words vector average over each tweet to be able to train a neural network and get a baseline result. Here are the results for the unprocessed tweets. \\\n",
        "We use FeaturesBuilder, an object we created, to build the embeddings average over the tweets. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W33cg0hIAI8r",
        "outputId": "5ff71565-794f-4b43-e28c-e7742f2be055"
      },
      "source": [
        "# Loading tweets and vocabulary\n",
        "vocab = load_vocabulary_pp(0)\n",
        "tweets_unprocessed = load_tweets_pp(0) \n",
        "\n",
        "# Load glove embedding\n",
        "word_vects = load_glove_embedding_pp(0)\n",
        "EMBEDDING_FEATURES = word_vects.shape[1]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loaded vocabulary containing 101298 words\n",
            "loaded 200000 tweets in dataframe with columns: Index(['text', 'label'], dtype='object')\n",
            "loaded glove embedding with shape (101298, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WY2TOeaODnrQ"
      },
      "source": [
        "# Create features_builder object\r\n",
        "features_builder = FeaturesBuilder(tweets_unprocessed, vocab, word_vects)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFDDkkouAI8r",
        "outputId": "4b0abdb3-4823-40e3-8d22-943fe5ee51c0"
      },
      "source": [
        "# Build the average of word vectors over each tweet \n",
        "\n",
        "X, y = features_builder.build_avg_tweet_embedding()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "built features with shape (199995, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-muYv-EAAI8s",
        "outputId": "ca5c05cf-a6a9-46a0-9a96-97db1080bd15"
      },
      "source": [
        "model = keras.Sequential(\n",
        "    [\n",
        "        layers.Dense(1, activation=\"sigmoid\", name=\"out\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.compile( optimizer=keras.optimizers.Adam(),\n",
        "               loss=keras.losses.BinaryCrossentropy(),\n",
        "               metrics=[keras.metrics.BinaryAccuracy()] )\n",
        "\n",
        "history = model.fit( X_train, y_train,\n",
        "                     batch_size=128, epochs=20,\n",
        "                     validation_data=(X_test.astype(\"float32\"), y_test.astype(\"float32\")) )\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1047/1047 [==============================] - 3s 3ms/step - loss: 0.6979 - binary_accuracy: 0.5094 - val_loss: 0.6910 - val_binary_accuracy: 0.5398\n",
            "Epoch 2/20\n",
            "1047/1047 [==============================] - 3s 3ms/step - loss: 0.6874 - binary_accuracy: 0.5570 - val_loss: 0.6840 - val_binary_accuracy: 0.5743\n",
            "Epoch 3/20\n",
            "1047/1047 [==============================] - 3s 3ms/step - loss: 0.6822 - binary_accuracy: 0.5753 - val_loss: 0.6802 - val_binary_accuracy: 0.5827\n",
            "Epoch 4/20\n",
            "1047/1047 [==============================] - 3s 3ms/step - loss: 0.6793 - binary_accuracy: 0.5820 - val_loss: 0.6779 - val_binary_accuracy: 0.5847\n",
            "Epoch 5/20\n",
            "1047/1047 [==============================] - 3s 3ms/step - loss: 0.6775 - binary_accuracy: 0.5834 - val_loss: 0.6766 - val_binary_accuracy: 0.5859\n",
            "Epoch 6/20\n",
            "1047/1047 [==============================] - 3s 3ms/step - loss: 0.6764 - binary_accuracy: 0.5835 - val_loss: 0.6756 - val_binary_accuracy: 0.5794\n",
            "Epoch 7/20\n",
            "1047/1047 [==============================] - 3s 3ms/step - loss: 0.6758 - binary_accuracy: 0.5831 - val_loss: 0.6750 - val_binary_accuracy: 0.5805\n",
            "Epoch 8/20\n",
            "1047/1047 [==============================] - 3s 3ms/step - loss: 0.6753 - binary_accuracy: 0.5824 - val_loss: 0.6747 - val_binary_accuracy: 0.5793\n",
            "Epoch 9/20\n",
            "1047/1047 [==============================] - 3s 3ms/step - loss: 0.6750 - binary_accuracy: 0.5827 - val_loss: 0.6744 - val_binary_accuracy: 0.5808\n",
            "Epoch 10/20\n",
            "1047/1047 [==============================] - 3s 3ms/step - loss: 0.6748 - binary_accuracy: 0.5820 - val_loss: 0.6743 - val_binary_accuracy: 0.5792\n",
            "Epoch 11/20\n",
            "1047/1047 [==============================] - 3s 3ms/step - loss: 0.6747 - binary_accuracy: 0.5816 - val_loss: 0.6742 - val_binary_accuracy: 0.5812\n",
            "Epoch 12/20\n",
            "1047/1047 [==============================] - 3s 3ms/step - loss: 0.6747 - binary_accuracy: 0.5819 - val_loss: 0.6742 - val_binary_accuracy: 0.5820\n",
            "Epoch 13/20\n",
            "1047/1047 [==============================] - 3s 3ms/step - loss: 0.6746 - binary_accuracy: 0.5817 - val_loss: 0.6742 - val_binary_accuracy: 0.5763\n",
            "Epoch 14/20\n",
            "1047/1047 [==============================] - 3s 3ms/step - loss: 0.6746 - binary_accuracy: 0.5807 - val_loss: 0.6741 - val_binary_accuracy: 0.5825\n",
            "Epoch 15/20\n",
            "1047/1047 [==============================] - 3s 3ms/step - loss: 0.6745 - binary_accuracy: 0.5818 - val_loss: 0.6740 - val_binary_accuracy: 0.5783\n",
            "Epoch 16/20\n",
            "1047/1047 [==============================] - 3s 3ms/step - loss: 0.6745 - binary_accuracy: 0.5815 - val_loss: 0.6741 - val_binary_accuracy: 0.5824\n",
            "Epoch 17/20\n",
            "1047/1047 [==============================] - 3s 3ms/step - loss: 0.6745 - binary_accuracy: 0.5812 - val_loss: 0.6741 - val_binary_accuracy: 0.5818\n",
            "Epoch 18/20\n",
            "1047/1047 [==============================] - 3s 3ms/step - loss: 0.6745 - binary_accuracy: 0.5814 - val_loss: 0.6740 - val_binary_accuracy: 0.5815\n",
            "Epoch 19/20\n",
            "1047/1047 [==============================] - 3s 3ms/step - loss: 0.6745 - binary_accuracy: 0.5814 - val_loss: 0.6740 - val_binary_accuracy: 0.5807\n",
            "Epoch 20/20\n",
            "1047/1047 [==============================] - 3s 3ms/step - loss: 0.6745 - binary_accuracy: 0.5810 - val_loss: 0.6742 - val_binary_accuracy: 0.5838\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "out (Dense)                  (None, 1)                 21        \n",
            "=================================================================\n",
            "Total params: 21\n",
            "Trainable params: 21\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUeUNIFvAI8s"
      },
      "source": [
        "We didn't spend that much time on optimizing the results for that option as we quickly found better results using word embedding sequences. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CDzV7YbAI8t"
      },
      "source": [
        "## 30 word embeddings sequence / tweet \n",
        "\n",
        "The more advanced approach on classifying the tweets was to use 30 word vectors per tweet, padding with null vectors when necessary. \\\n",
        "We use our own FeatureBuilder to create the padded sequences. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bRpcGz0AI8t"
      },
      "source": [
        "### Unprocessed tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPi39e35AI8t"
      },
      "source": [
        "#create features_builder object\n",
        "features_builder = FeaturesBuilder(tweets_unprocessed, vocab, word_vects)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdLvqIBpAI8u",
        "outputId": "f62f7a17-8ff4-4283-cdfc-549b8a9b1c3c"
      },
      "source": [
        "# Build the 30 word vectors sequences per tweet \n",
        "\n",
        "X, y = features_builder.build_word_embedding_sequences()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "built features with shape (200000, 30, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQW2Y1Tz0H2V",
        "outputId": "2d2f80b1-2609-4e1c-a558-e988f99e0077"
      },
      "source": [
        "model = keras.Sequential(\r\n",
        "    [\r\n",
        "        layers.Bidirectional(layers.LSTM(20, name=\"lstm\", return_sequences=True)),\r\n",
        "        layers.Bidirectional(layers.LSTM(40, name=\"lstm\", return_sequences=True)),\r\n",
        "        layers.Bidirectional(layers.LSTM(20, name=\"lstm\")),\r\n",
        "\r\n",
        "        layers.Dense(1, activation=\"sigmoid\", name=\"out\"),\r\n",
        "    ]\r\n",
        ")\r\n",
        "\r\n",
        "model.compile(\r\n",
        "    optimizer=keras.optimizers.Adam(),\r\n",
        "    loss=keras.losses.BinaryCrossentropy(),\r\n",
        "    metrics=[keras.metrics.BinaryAccuracy()],\r\n",
        ")\r\n",
        "\r\n",
        "history = model.fit(\r\n",
        "    X_train,\r\n",
        "    y_train,\r\n",
        "    batch_size=64,\r\n",
        "    epochs=40,\r\n",
        "    verbose=2, \r\n",
        "    # We pass some validation for\r\n",
        "    # monitoring validation loss and metrics\r\n",
        "    # at the end of each epoch\r\n",
        "    validation_data=(X_test.astype(\"float32\"), y_test.astype(\"float32\")),\r\n",
        ")\r\n",
        "\r\n",
        "print(model.summary())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "2094/2094 - 41s - loss: 0.6080 - binary_accuracy: 0.6368 - val_loss: 0.5943 - val_binary_accuracy: 0.6421\n",
            "Epoch 2/40\n",
            "2094/2094 - 40s - loss: 0.5885 - binary_accuracy: 0.6482 - val_loss: 0.5856 - val_binary_accuracy: 0.6464\n",
            "Epoch 3/40\n",
            "2094/2094 - 40s - loss: 0.5808 - binary_accuracy: 0.6532 - val_loss: 0.5795 - val_binary_accuracy: 0.6526\n",
            "Epoch 4/40\n",
            "2094/2094 - 40s - loss: 0.5743 - binary_accuracy: 0.6588 - val_loss: 0.5754 - val_binary_accuracy: 0.6559\n",
            "Epoch 5/40\n",
            "2094/2094 - 40s - loss: 0.5677 - binary_accuracy: 0.6636 - val_loss: 0.5747 - val_binary_accuracy: 0.6584\n",
            "Epoch 6/40\n",
            "2094/2094 - 40s - loss: 0.5609 - binary_accuracy: 0.6710 - val_loss: 0.5653 - val_binary_accuracy: 0.6661\n",
            "Epoch 7/40\n",
            "2094/2094 - 40s - loss: 0.5544 - binary_accuracy: 0.6772 - val_loss: 0.5702 - val_binary_accuracy: 0.6518\n",
            "Epoch 8/40\n",
            "2094/2094 - 40s - loss: 0.5478 - binary_accuracy: 0.6846 - val_loss: 0.5590 - val_binary_accuracy: 0.6726\n",
            "Epoch 9/40\n",
            "2094/2094 - 39s - loss: 0.5409 - binary_accuracy: 0.6907 - val_loss: 0.5564 - val_binary_accuracy: 0.6768\n",
            "Epoch 10/40\n",
            "2094/2094 - 39s - loss: 0.5350 - binary_accuracy: 0.6963 - val_loss: 0.5534 - val_binary_accuracy: 0.6804\n",
            "Epoch 11/40\n",
            "2094/2094 - 39s - loss: 0.5287 - binary_accuracy: 0.7018 - val_loss: 0.5571 - val_binary_accuracy: 0.6753\n",
            "Epoch 12/40\n",
            "2094/2094 - 40s - loss: 0.5227 - binary_accuracy: 0.7079 - val_loss: 0.5511 - val_binary_accuracy: 0.6885\n",
            "Epoch 13/40\n",
            "2094/2094 - 39s - loss: 0.5159 - binary_accuracy: 0.7136 - val_loss: 0.5500 - val_binary_accuracy: 0.6898\n",
            "Epoch 14/40\n",
            "2094/2094 - 39s - loss: 0.5095 - binary_accuracy: 0.7177 - val_loss: 0.5490 - val_binary_accuracy: 0.6938\n",
            "Epoch 15/40\n",
            "2094/2094 - 39s - loss: 0.5033 - binary_accuracy: 0.7241 - val_loss: 0.5481 - val_binary_accuracy: 0.6955\n",
            "Epoch 16/40\n",
            "2094/2094 - 39s - loss: 0.4975 - binary_accuracy: 0.7290 - val_loss: 0.5511 - val_binary_accuracy: 0.6948\n",
            "Epoch 17/40\n",
            "2094/2094 - 39s - loss: 0.4910 - binary_accuracy: 0.7335 - val_loss: 0.5505 - val_binary_accuracy: 0.6949\n",
            "Epoch 18/40\n",
            "2094/2094 - 39s - loss: 0.4859 - binary_accuracy: 0.7384 - val_loss: 0.5520 - val_binary_accuracy: 0.6993\n",
            "Epoch 19/40\n",
            "2094/2094 - 39s - loss: 0.4804 - binary_accuracy: 0.7427 - val_loss: 0.5555 - val_binary_accuracy: 0.7009\n",
            "Epoch 20/40\n",
            "2094/2094 - 40s - loss: 0.4745 - binary_accuracy: 0.7483 - val_loss: 0.5583 - val_binary_accuracy: 0.6947\n",
            "Epoch 21/40\n",
            "2094/2094 - 39s - loss: 0.4693 - binary_accuracy: 0.7505 - val_loss: 0.5542 - val_binary_accuracy: 0.7033\n",
            "Epoch 22/40\n",
            "2094/2094 - 39s - loss: 0.4636 - binary_accuracy: 0.7560 - val_loss: 0.5610 - val_binary_accuracy: 0.7054\n",
            "Epoch 23/40\n",
            "2094/2094 - 39s - loss: 0.4580 - binary_accuracy: 0.7583 - val_loss: 0.5726 - val_binary_accuracy: 0.6973\n",
            "Epoch 24/40\n",
            "2094/2094 - 39s - loss: 0.4534 - binary_accuracy: 0.7629 - val_loss: 0.5631 - val_binary_accuracy: 0.7016\n",
            "Epoch 25/40\n",
            "2094/2094 - 39s - loss: 0.4483 - binary_accuracy: 0.7655 - val_loss: 0.5667 - val_binary_accuracy: 0.7045\n",
            "Epoch 26/40\n",
            "2094/2094 - 38s - loss: 0.4430 - binary_accuracy: 0.7685 - val_loss: 0.5743 - val_binary_accuracy: 0.7032\n",
            "Epoch 27/40\n",
            "2094/2094 - 39s - loss: 0.4374 - binary_accuracy: 0.7731 - val_loss: 0.5799 - val_binary_accuracy: 0.7060\n",
            "Epoch 28/40\n",
            "2094/2094 - 39s - loss: 0.4326 - binary_accuracy: 0.7768 - val_loss: 0.5866 - val_binary_accuracy: 0.7043\n",
            "Epoch 29/40\n",
            "2094/2094 - 39s - loss: 0.4283 - binary_accuracy: 0.7796 - val_loss: 0.6009 - val_binary_accuracy: 0.7016\n",
            "Epoch 30/40\n",
            "2094/2094 - 39s - loss: 0.4236 - binary_accuracy: 0.7823 - val_loss: 0.6150 - val_binary_accuracy: 0.7020\n",
            "Epoch 31/40\n",
            "2094/2094 - 38s - loss: 0.4191 - binary_accuracy: 0.7851 - val_loss: 0.6125 - val_binary_accuracy: 0.7067\n",
            "Epoch 32/40\n",
            "2094/2094 - 38s - loss: 0.4149 - binary_accuracy: 0.7876 - val_loss: 0.6077 - val_binary_accuracy: 0.7046\n",
            "Epoch 33/40\n",
            "2094/2094 - 37s - loss: 0.4098 - binary_accuracy: 0.7908 - val_loss: 0.6307 - val_binary_accuracy: 0.7022\n",
            "Epoch 34/40\n",
            "2094/2094 - 38s - loss: 0.4065 - binary_accuracy: 0.7929 - val_loss: 0.6254 - val_binary_accuracy: 0.7042\n",
            "Epoch 35/40\n",
            "2094/2094 - 37s - loss: 0.4019 - binary_accuracy: 0.7962 - val_loss: 0.6427 - val_binary_accuracy: 0.7057\n",
            "Epoch 36/40\n",
            "2094/2094 - 39s - loss: 0.3985 - binary_accuracy: 0.7982 - val_loss: 0.6409 - val_binary_accuracy: 0.7045\n",
            "Epoch 37/40\n",
            "2094/2094 - 37s - loss: 0.3940 - binary_accuracy: 0.8013 - val_loss: 0.6488 - val_binary_accuracy: 0.6986\n",
            "Epoch 38/40\n",
            "2094/2094 - 38s - loss: 0.3893 - binary_accuracy: 0.8037 - val_loss: 0.6512 - val_binary_accuracy: 0.7020\n",
            "Epoch 39/40\n",
            "2094/2094 - 38s - loss: 0.3858 - binary_accuracy: 0.8059 - val_loss: 0.6584 - val_binary_accuracy: 0.7018\n",
            "Epoch 40/40\n",
            "2094/2094 - 37s - loss: 0.3815 - binary_accuracy: 0.8084 - val_loss: 0.6543 - val_binary_accuracy: 0.7025\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional (Bidirectional (None, 30, 40)            6560      \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 30, 80)            25920     \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 40)                16160     \n",
            "_________________________________________________________________\n",
            "out (Dense)                  (None, 1)                 41        \n",
            "=================================================================\n",
            "Total params: 48,681\n",
            "Trainable params: 48,681\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdP5yzzaAI8v"
      },
      "source": [
        " # Saving the model results in a file \n",
        "with open('models/model_history_pp0', 'wb') as file:\n",
        "    pkl.dump(history.history, file)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8rPZa7hAI8v"
      },
      "source": [
        "### Preprocessing option 1 - Add tokens "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25339qJSAI8v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97299570-64c0-4675-f6e9-f563949e3995"
      },
      "source": [
        "# Loading tweets and vocabulary\n",
        "vocab = load_vocabulary_pp(1)\n",
        "tweets_pp1 = load_tweets_pp(1) \n",
        "\n",
        "# Load glove embedding\n",
        "word_vects = load_glove_embedding_pp(1)\n",
        "EMBEDDING_FEATURES = word_vects.shape[1]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loaded vocabulary containing 92334 words\n",
            "loaded 200000 tweets in dataframe with columns: Index(['text', 'label'], dtype='object')\n",
            "loaded glove embedding with shape (92334, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Dh8dOunAI8v"
      },
      "source": [
        "#create features_builder object\n",
        "features_builder = FeaturesBuilder(tweets_pp1, vocab, word_vects)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnJIicPpAI8w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b640cea0-3fcb-46d6-88e3-44b7a75c1480"
      },
      "source": [
        "# Build the 30 word vectors sequences per tweet \n",
        "\n",
        "X, y = features_builder.build_word_embedding_sequences()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "built features with shape (200000, 30, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdE-bYPoAI8w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25880f82-cc27-4373-c890-495defaff4a1"
      },
      "source": [
        "model = keras.Sequential(\n",
        "    [\n",
        "        layers.Bidirectional(layers.LSTM(20, name=\"lstm\", return_sequences=True)),\n",
        "        layers.Bidirectional(layers.LSTM(40, name=\"lstm\", return_sequences=True)),\n",
        "        layers.Bidirectional(layers.LSTM(20, name=\"lstm\")),\n",
        "        layers.Dense(1, activation=\"sigmoid\", name=\"out\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss=keras.losses.BinaryCrossentropy(),\n",
        "    metrics=[keras.metrics.BinaryAccuracy()],\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    batch_size=64,\n",
        "    epochs=30,\n",
        "    # We pass some validation for\n",
        "    # monitoring validation loss and metrics\n",
        "    # at the end of each epoch\n",
        "    validation_data=(X_test.astype(\"float32\"), y_test.astype(\"float32\")),\n",
        ")\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "2094/2094 [==============================] - 41s 19ms/step - loss: 0.6025 - binary_accuracy: 0.6399 - val_loss: 0.5965 - val_binary_accuracy: 0.6397\n",
            "Epoch 2/30\n",
            "2094/2094 [==============================] - 40s 19ms/step - loss: 0.5912 - binary_accuracy: 0.6469 - val_loss: 0.5923 - val_binary_accuracy: 0.6416\n",
            "Epoch 3/30\n",
            "2094/2094 [==============================] - 40s 19ms/step - loss: 0.5857 - binary_accuracy: 0.6502 - val_loss: 0.5840 - val_binary_accuracy: 0.6490\n",
            "Epoch 4/30\n",
            "2094/2094 [==============================] - 40s 19ms/step - loss: 0.5781 - binary_accuracy: 0.6553 - val_loss: 0.5784 - val_binary_accuracy: 0.6525\n",
            "Epoch 5/30\n",
            "2094/2094 [==============================] - 40s 19ms/step - loss: 0.5713 - binary_accuracy: 0.6623 - val_loss: 0.5718 - val_binary_accuracy: 0.6598\n",
            "Epoch 6/30\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.5640 - binary_accuracy: 0.6699 - val_loss: 0.5666 - val_binary_accuracy: 0.6663\n",
            "Epoch 7/30\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.5566 - binary_accuracy: 0.6790 - val_loss: 0.5658 - val_binary_accuracy: 0.6678\n",
            "Epoch 8/30\n",
            "2094/2094 [==============================] - 40s 19ms/step - loss: 0.5501 - binary_accuracy: 0.6848 - val_loss: 0.5621 - val_binary_accuracy: 0.6762\n",
            "Epoch 9/30\n",
            "2094/2094 [==============================] - 40s 19ms/step - loss: 0.5428 - binary_accuracy: 0.6914 - val_loss: 0.5556 - val_binary_accuracy: 0.6800\n",
            "Epoch 10/30\n",
            "2094/2094 [==============================] - 40s 19ms/step - loss: 0.5362 - binary_accuracy: 0.6971 - val_loss: 0.5542 - val_binary_accuracy: 0.6845\n",
            "Epoch 11/30\n",
            "2094/2094 [==============================] - 40s 19ms/step - loss: 0.5302 - binary_accuracy: 0.7044 - val_loss: 0.5500 - val_binary_accuracy: 0.6890\n",
            "Epoch 12/30\n",
            "2094/2094 [==============================] - 40s 19ms/step - loss: 0.5231 - binary_accuracy: 0.7088 - val_loss: 0.5490 - val_binary_accuracy: 0.6896\n",
            "Epoch 13/30\n",
            "2094/2094 [==============================] - 40s 19ms/step - loss: 0.5177 - binary_accuracy: 0.7128 - val_loss: 0.5454 - val_binary_accuracy: 0.6923\n",
            "Epoch 14/30\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.5113 - binary_accuracy: 0.7203 - val_loss: 0.5474 - val_binary_accuracy: 0.6929\n",
            "Epoch 15/30\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.5051 - binary_accuracy: 0.7250 - val_loss: 0.5450 - val_binary_accuracy: 0.6973\n",
            "Epoch 16/30\n",
            "2094/2094 [==============================] - 40s 19ms/step - loss: 0.4982 - binary_accuracy: 0.7297 - val_loss: 0.5449 - val_binary_accuracy: 0.7000\n",
            "Epoch 17/30\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.4921 - binary_accuracy: 0.7345 - val_loss: 0.5461 - val_binary_accuracy: 0.7025\n",
            "Epoch 18/30\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.4868 - binary_accuracy: 0.7390 - val_loss: 0.5485 - val_binary_accuracy: 0.7023\n",
            "Epoch 19/30\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.4799 - binary_accuracy: 0.7442 - val_loss: 0.5477 - val_binary_accuracy: 0.7029\n",
            "Epoch 20/30\n",
            "2094/2094 [==============================] - 40s 19ms/step - loss: 0.4743 - binary_accuracy: 0.7476 - val_loss: 0.5497 - val_binary_accuracy: 0.7037\n",
            "Epoch 21/30\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.4683 - binary_accuracy: 0.7522 - val_loss: 0.5497 - val_binary_accuracy: 0.7056\n",
            "Epoch 22/30\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.4644 - binary_accuracy: 0.7555 - val_loss: 0.5612 - val_binary_accuracy: 0.7034\n",
            "Epoch 23/30\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.4579 - binary_accuracy: 0.7599 - val_loss: 0.5483 - val_binary_accuracy: 0.7096\n",
            "Epoch 24/30\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.4520 - binary_accuracy: 0.7633 - val_loss: 0.5636 - val_binary_accuracy: 0.7081\n",
            "Epoch 25/30\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.4473 - binary_accuracy: 0.7661 - val_loss: 0.5556 - val_binary_accuracy: 0.7087\n",
            "Epoch 26/30\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.4424 - binary_accuracy: 0.7698 - val_loss: 0.5702 - val_binary_accuracy: 0.7129\n",
            "Epoch 27/30\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.4376 - binary_accuracy: 0.7732 - val_loss: 0.5693 - val_binary_accuracy: 0.7091\n",
            "Epoch 28/30\n",
            "2094/2094 [==============================] - 40s 19ms/step - loss: 0.4321 - binary_accuracy: 0.7767 - val_loss: 0.5728 - val_binary_accuracy: 0.7127\n",
            "Epoch 29/30\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.4272 - binary_accuracy: 0.7811 - val_loss: 0.5860 - val_binary_accuracy: 0.7043\n",
            "Epoch 30/30\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.4230 - binary_accuracy: 0.7830 - val_loss: 0.5890 - val_binary_accuracy: 0.7138\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_3 (Bidirection (None, 30, 40)            6560      \n",
            "_________________________________________________________________\n",
            "bidirectional_4 (Bidirection (None, 30, 80)            25920     \n",
            "_________________________________________________________________\n",
            "bidirectional_5 (Bidirection (None, 40)                16160     \n",
            "_________________________________________________________________\n",
            "out (Dense)                  (None, 1)                 41        \n",
            "=================================================================\n",
            "Total params: 48,681\n",
            "Trainable params: 48,681\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xmzh_AqHAI8w"
      },
      "source": [
        "# Saving the model results in a file \n",
        "with open('models/model_history_pp1', 'wb') as file:\n",
        "    pkl.dump(history.history, file)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KERBCyXRAI8x"
      },
      "source": [
        "### Preprocessing option 2 - Tokens + Hagtag Split "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sze9U_c1AI8x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1ef9361-1a42-455d-c36e-8e5d57a62cb8"
      },
      "source": [
        "# Loading tweets and vocabulary\n",
        "vocab = load_vocabulary_pp(2)\n",
        "tweets_pp2 = load_tweets_pp(2) \n",
        "\n",
        "# Load glove embedding\n",
        "word_vects = load_glove_embedding_pp(2)\n",
        "EMBEDDING_FEATURES = word_vects.shape[1]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loaded vocabulary containing 92335 words\n",
            "loaded 200000 tweets in dataframe with columns: Index(['text', 'label'], dtype='object')\n",
            "loaded glove embedding with shape (92335, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwOv7Z3UAI8y"
      },
      "source": [
        "#create features_builder object\n",
        "features_builder = FeaturesBuilder(tweets_pp2, vocab, word_vects)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEGx8v4xAI8y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23ff5e0d-f229-47b7-fc56-e605b9c7af70"
      },
      "source": [
        "# Build the 30 word vectors sequences per tweet \n",
        "\n",
        "X, y = features_builder.build_word_embedding_sequences()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "built features with shape (200000, 30, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIa39BCuAI8y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56389e4d-c324-4f29-dd4e-7ffa5fa1a7eb"
      },
      "source": [
        "model = keras.Sequential(\n",
        "    [\n",
        "        layers.Bidirectional(layers.LSTM(20, name=\"lstm\", return_sequences=True)),\n",
        "        layers.Bidirectional(layers.LSTM(40, name=\"lstm\", return_sequences=True)),\n",
        "        layers.Bidirectional(layers.LSTM(20, name=\"lstm\")),\n",
        "        layers.Dense(1, activation=\"sigmoid\", name=\"out\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss=keras.losses.BinaryCrossentropy(),\n",
        "    metrics=[keras.metrics.BinaryAccuracy()],\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    batch_size=64,\n",
        "    epochs=40,\n",
        "    # We pass some validation for\n",
        "    # monitoring validation loss and metrics\n",
        "    # at the end of each epoch\n",
        "    validation_data=(X_test.astype(\"float32\"), y_test.astype(\"float32\")),\n",
        ")\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "2094/2094 [==============================] - 40s 19ms/step - loss: 0.6001 - binary_accuracy: 0.6414 - val_loss: 0.5937 - val_binary_accuracy: 0.6431\n",
            "Epoch 2/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.5900 - binary_accuracy: 0.6487 - val_loss: 0.5932 - val_binary_accuracy: 0.6372\n",
            "Epoch 3/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.5852 - binary_accuracy: 0.6508 - val_loss: 0.5846 - val_binary_accuracy: 0.6469\n",
            "Epoch 4/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.5792 - binary_accuracy: 0.6534 - val_loss: 0.5822 - val_binary_accuracy: 0.6491\n",
            "Epoch 5/40\n",
            "2094/2094 [==============================] - 40s 19ms/step - loss: 0.5742 - binary_accuracy: 0.6572 - val_loss: 0.5783 - val_binary_accuracy: 0.6508\n",
            "Epoch 6/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.5695 - binary_accuracy: 0.6606 - val_loss: 0.5771 - val_binary_accuracy: 0.6515\n",
            "Epoch 7/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.5654 - binary_accuracy: 0.6641 - val_loss: 0.5761 - val_binary_accuracy: 0.6548\n",
            "Epoch 8/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.5611 - binary_accuracy: 0.6682 - val_loss: 0.5775 - val_binary_accuracy: 0.6592\n",
            "Epoch 9/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.5557 - binary_accuracy: 0.6749 - val_loss: 0.5704 - val_binary_accuracy: 0.6595\n",
            "Epoch 10/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.5499 - binary_accuracy: 0.6801 - val_loss: 0.5676 - val_binary_accuracy: 0.6669\n",
            "Epoch 11/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.5441 - binary_accuracy: 0.6853 - val_loss: 0.5669 - val_binary_accuracy: 0.6680\n",
            "Epoch 12/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.5379 - binary_accuracy: 0.6921 - val_loss: 0.5727 - val_binary_accuracy: 0.6721\n",
            "Epoch 13/40\n",
            "2094/2094 [==============================] - 40s 19ms/step - loss: 0.5314 - binary_accuracy: 0.6973 - val_loss: 0.5656 - val_binary_accuracy: 0.6769\n",
            "Epoch 14/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.5253 - binary_accuracy: 0.7027 - val_loss: 0.5613 - val_binary_accuracy: 0.6790\n",
            "Epoch 15/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.5195 - binary_accuracy: 0.7084 - val_loss: 0.5605 - val_binary_accuracy: 0.6803\n",
            "Epoch 16/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.5136 - binary_accuracy: 0.7131 - val_loss: 0.5624 - val_binary_accuracy: 0.6805\n",
            "Epoch 17/40\n",
            "2094/2094 [==============================] - 40s 19ms/step - loss: 0.5068 - binary_accuracy: 0.7188 - val_loss: 0.5627 - val_binary_accuracy: 0.6827\n",
            "Epoch 18/40\n",
            "2094/2094 [==============================] - 41s 19ms/step - loss: 0.5004 - binary_accuracy: 0.7245 - val_loss: 0.5668 - val_binary_accuracy: 0.6842\n",
            "Epoch 19/40\n",
            "2094/2094 [==============================] - 40s 19ms/step - loss: 0.4941 - binary_accuracy: 0.7304 - val_loss: 0.5711 - val_binary_accuracy: 0.6822\n",
            "Epoch 20/40\n",
            "2094/2094 [==============================] - 40s 19ms/step - loss: 0.4891 - binary_accuracy: 0.7337 - val_loss: 0.5688 - val_binary_accuracy: 0.6893\n",
            "Epoch 21/40\n",
            "2094/2094 [==============================] - 41s 19ms/step - loss: 0.4822 - binary_accuracy: 0.7399 - val_loss: 0.5646 - val_binary_accuracy: 0.6906\n",
            "Epoch 22/40\n",
            "2094/2094 [==============================] - 40s 19ms/step - loss: 0.4763 - binary_accuracy: 0.7430 - val_loss: 0.5676 - val_binary_accuracy: 0.6905\n",
            "Epoch 23/40\n",
            "2094/2094 [==============================] - 40s 19ms/step - loss: 0.4702 - binary_accuracy: 0.7485 - val_loss: 0.5752 - val_binary_accuracy: 0.6921\n",
            "Epoch 24/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.4639 - binary_accuracy: 0.7537 - val_loss: 0.5776 - val_binary_accuracy: 0.6939\n",
            "Epoch 25/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.4586 - binary_accuracy: 0.7571 - val_loss: 0.5854 - val_binary_accuracy: 0.6927\n",
            "Epoch 26/40\n",
            "2094/2094 [==============================] - 40s 19ms/step - loss: 0.4536 - binary_accuracy: 0.7603 - val_loss: 0.5977 - val_binary_accuracy: 0.6957\n",
            "Epoch 27/40\n",
            "2094/2094 [==============================] - 40s 19ms/step - loss: 0.4480 - binary_accuracy: 0.7635 - val_loss: 0.6041 - val_binary_accuracy: 0.6912\n",
            "Epoch 28/40\n",
            "2094/2094 [==============================] - 40s 19ms/step - loss: 0.4425 - binary_accuracy: 0.7679 - val_loss: 0.5919 - val_binary_accuracy: 0.6918\n",
            "Epoch 29/40\n",
            "2094/2094 [==============================] - 40s 19ms/step - loss: 0.4381 - binary_accuracy: 0.7714 - val_loss: 0.6135 - val_binary_accuracy: 0.6953\n",
            "Epoch 30/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.4319 - binary_accuracy: 0.7751 - val_loss: 0.6019 - val_binary_accuracy: 0.6954\n",
            "Epoch 31/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.4275 - binary_accuracy: 0.7778 - val_loss: 0.6189 - val_binary_accuracy: 0.6965\n",
            "Epoch 32/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.4223 - binary_accuracy: 0.7817 - val_loss: 0.6260 - val_binary_accuracy: 0.6960\n",
            "Epoch 33/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.4196 - binary_accuracy: 0.7835 - val_loss: 0.6339 - val_binary_accuracy: 0.6978\n",
            "Epoch 34/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.4135 - binary_accuracy: 0.7867 - val_loss: 0.6263 - val_binary_accuracy: 0.6960\n",
            "Epoch 35/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.4099 - binary_accuracy: 0.7903 - val_loss: 0.6467 - val_binary_accuracy: 0.6977\n",
            "Epoch 36/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.4048 - binary_accuracy: 0.7927 - val_loss: 0.6587 - val_binary_accuracy: 0.7000\n",
            "Epoch 37/40\n",
            "2094/2094 [==============================] - 40s 19ms/step - loss: 0.3986 - binary_accuracy: 0.7953 - val_loss: 0.6571 - val_binary_accuracy: 0.6948\n",
            "Epoch 38/40\n",
            "2094/2094 [==============================] - 40s 19ms/step - loss: 0.3964 - binary_accuracy: 0.7984 - val_loss: 0.6533 - val_binary_accuracy: 0.6964\n",
            "Epoch 39/40\n",
            "2094/2094 [==============================] - 42s 20ms/step - loss: 0.3917 - binary_accuracy: 0.8002 - val_loss: 0.6747 - val_binary_accuracy: 0.6933\n",
            "Epoch 40/40\n",
            "2094/2094 [==============================] - 40s 19ms/step - loss: 0.3873 - binary_accuracy: 0.8038 - val_loss: 0.6866 - val_binary_accuracy: 0.6939\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_6 (Bidirection (None, 30, 40)            6560      \n",
            "_________________________________________________________________\n",
            "bidirectional_7 (Bidirection (None, 30, 80)            25920     \n",
            "_________________________________________________________________\n",
            "bidirectional_8 (Bidirection (None, 40)                16160     \n",
            "_________________________________________________________________\n",
            "out (Dense)                  (None, 1)                 41        \n",
            "=================================================================\n",
            "Total params: 48,681\n",
            "Trainable params: 48,681\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLlyxnPAAI8z"
      },
      "source": [
        "# Saving the model results in a file \n",
        "with open('models/model_history_pp2', 'wb') as file:\n",
        "    pkl.dump(history.history, file)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOwVtJ4kAI8z"
      },
      "source": [
        "### Preprocessing option 3 - Tokens + Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35hQshnFAI8z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4109859-27f4-4c64-9c11-6cb4339357b8"
      },
      "source": [
        "# Loading tweets and vocabulary\n",
        "vocab = load_vocabulary_pp(3)\n",
        "tweets_pp1 = load_tweets_pp(3) \n",
        "\n",
        "# Load glove embedding\n",
        "word_vects = load_glove_embedding_pp(3)\n",
        "EMBEDDING_FEATURES = word_vects.shape[1]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loaded vocabulary containing 92335 words\n",
            "loaded 200000 tweets in dataframe with columns: Index(['text', 'label'], dtype='object')\n",
            "loaded glove embedding with shape (92335, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKE6eArxAI8z"
      },
      "source": [
        "#create features_builder object\n",
        "features_builder = FeaturesBuilder(tweets_pp2, vocab, word_vects)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92PUqoizAI80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e15a753d-d9fd-478c-e408-ce8e4ef3b772"
      },
      "source": [
        "# Build the 30 word vectors sequences per tweet \n",
        "\n",
        "X, y = features_builder.build_word_embedding_sequences()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "built features with shape (200000, 30, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPtqYAB7AI80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c066e37a-11c5-4ca6-a633-937528880f36"
      },
      "source": [
        "model = keras.Sequential(\n",
        "    [\n",
        "        layers.Bidirectional(layers.LSTM(20, name=\"lstm\", return_sequences=True)),\n",
        "        layers.Bidirectional(layers.LSTM(40, name=\"lstm\", return_sequences=True)),\n",
        "        layers.Bidirectional(layers.LSTM(20, name=\"lstm\")),\n",
        "        layers.Dense(1, activation=\"sigmoid\", name=\"out\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss=keras.losses.BinaryCrossentropy(),\n",
        "    metrics=[keras.metrics.BinaryAccuracy()],\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    batch_size=64,\n",
        "    epochs=40,\n",
        "    # We pass some validation for\n",
        "    # monitoring validation loss and metrics\n",
        "    # at the end of each epoch\n",
        "    validation_data=(X_test.astype(\"float32\"), y_test.astype(\"float32\")),\n",
        ")\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "2094/2094 [==============================] - 42s 20ms/step - loss: 0.5837 - binary_accuracy: 0.6649 - val_loss: 0.5635 - val_binary_accuracy: 0.6786\n",
            "Epoch 2/40\n",
            "2094/2094 [==============================] - 40s 19ms/step - loss: 0.5555 - binary_accuracy: 0.6877 - val_loss: 0.5526 - val_binary_accuracy: 0.6877\n",
            "Epoch 3/40\n",
            "2094/2094 [==============================] - 40s 19ms/step - loss: 0.5441 - binary_accuracy: 0.6977 - val_loss: 0.5435 - val_binary_accuracy: 0.6963\n",
            "Epoch 4/40\n",
            "2094/2094 [==============================] - 41s 19ms/step - loss: 0.5352 - binary_accuracy: 0.7054 - val_loss: 0.5412 - val_binary_accuracy: 0.6986\n",
            "Epoch 5/40\n",
            "2094/2094 [==============================] - 40s 19ms/step - loss: 0.5278 - binary_accuracy: 0.7107 - val_loss: 0.5354 - val_binary_accuracy: 0.7040\n",
            "Epoch 6/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.5212 - binary_accuracy: 0.7164 - val_loss: 0.5321 - val_binary_accuracy: 0.7085\n",
            "Epoch 7/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.5142 - binary_accuracy: 0.7215 - val_loss: 0.5315 - val_binary_accuracy: 0.7086\n",
            "Epoch 8/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.5080 - binary_accuracy: 0.7258 - val_loss: 0.5283 - val_binary_accuracy: 0.7127\n",
            "Epoch 9/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.5010 - binary_accuracy: 0.7315 - val_loss: 0.5299 - val_binary_accuracy: 0.7097\n",
            "Epoch 10/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.4946 - binary_accuracy: 0.7372 - val_loss: 0.5279 - val_binary_accuracy: 0.7149\n",
            "Epoch 11/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.4879 - binary_accuracy: 0.7421 - val_loss: 0.5228 - val_binary_accuracy: 0.7188\n",
            "Epoch 12/40\n",
            "2094/2094 [==============================] - 40s 19ms/step - loss: 0.4809 - binary_accuracy: 0.7474 - val_loss: 0.5265 - val_binary_accuracy: 0.7158\n",
            "Epoch 13/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.4742 - binary_accuracy: 0.7532 - val_loss: 0.5236 - val_binary_accuracy: 0.7181\n",
            "Epoch 14/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.4666 - binary_accuracy: 0.7588 - val_loss: 0.5285 - val_binary_accuracy: 0.7170\n",
            "Epoch 15/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.4600 - binary_accuracy: 0.7630 - val_loss: 0.5390 - val_binary_accuracy: 0.7194\n",
            "Epoch 16/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.4534 - binary_accuracy: 0.7669 - val_loss: 0.5402 - val_binary_accuracy: 0.7162\n",
            "Epoch 17/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.4468 - binary_accuracy: 0.7723 - val_loss: 0.5413 - val_binary_accuracy: 0.7175\n",
            "Epoch 18/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.4396 - binary_accuracy: 0.7774 - val_loss: 0.5430 - val_binary_accuracy: 0.7174\n",
            "Epoch 19/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.4328 - binary_accuracy: 0.7808 - val_loss: 0.5551 - val_binary_accuracy: 0.7181\n",
            "Epoch 20/40\n",
            "2094/2094 [==============================] - 40s 19ms/step - loss: 0.4266 - binary_accuracy: 0.7868 - val_loss: 0.5594 - val_binary_accuracy: 0.7174\n",
            "Epoch 21/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.4190 - binary_accuracy: 0.7913 - val_loss: 0.5637 - val_binary_accuracy: 0.7117\n",
            "Epoch 22/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.4121 - binary_accuracy: 0.7953 - val_loss: 0.5739 - val_binary_accuracy: 0.7146\n",
            "Epoch 23/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.4059 - binary_accuracy: 0.7989 - val_loss: 0.5812 - val_binary_accuracy: 0.7170\n",
            "Epoch 24/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.3996 - binary_accuracy: 0.8030 - val_loss: 0.5912 - val_binary_accuracy: 0.7151\n",
            "Epoch 25/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.3937 - binary_accuracy: 0.8052 - val_loss: 0.5937 - val_binary_accuracy: 0.7144\n",
            "Epoch 26/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.3876 - binary_accuracy: 0.8096 - val_loss: 0.6089 - val_binary_accuracy: 0.7165\n",
            "Epoch 27/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.3810 - binary_accuracy: 0.8135 - val_loss: 0.6200 - val_binary_accuracy: 0.7155\n",
            "Epoch 28/40\n",
            "2094/2094 [==============================] - 40s 19ms/step - loss: 0.3762 - binary_accuracy: 0.8172 - val_loss: 0.6216 - val_binary_accuracy: 0.7125\n",
            "Epoch 29/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.3709 - binary_accuracy: 0.8203 - val_loss: 0.6262 - val_binary_accuracy: 0.7097\n",
            "Epoch 30/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.3655 - binary_accuracy: 0.8239 - val_loss: 0.6470 - val_binary_accuracy: 0.7112\n",
            "Epoch 31/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.3594 - binary_accuracy: 0.8278 - val_loss: 0.6528 - val_binary_accuracy: 0.7107\n",
            "Epoch 32/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.3556 - binary_accuracy: 0.8296 - val_loss: 0.6500 - val_binary_accuracy: 0.7103\n",
            "Epoch 33/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.3507 - binary_accuracy: 0.8324 - val_loss: 0.6697 - val_binary_accuracy: 0.7077\n",
            "Epoch 34/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.3464 - binary_accuracy: 0.8354 - val_loss: 0.6750 - val_binary_accuracy: 0.7104\n",
            "Epoch 35/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.3405 - binary_accuracy: 0.8384 - val_loss: 0.6779 - val_binary_accuracy: 0.7101\n",
            "Epoch 36/40\n",
            "2094/2094 [==============================] - 40s 19ms/step - loss: 0.3374 - binary_accuracy: 0.8408 - val_loss: 0.7013 - val_binary_accuracy: 0.7081\n",
            "Epoch 37/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.3317 - binary_accuracy: 0.8433 - val_loss: 0.7074 - val_binary_accuracy: 0.7090\n",
            "Epoch 38/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.3275 - binary_accuracy: 0.8461 - val_loss: 0.7379 - val_binary_accuracy: 0.7095\n",
            "Epoch 39/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.3239 - binary_accuracy: 0.8477 - val_loss: 0.7186 - val_binary_accuracy: 0.7035\n",
            "Epoch 40/40\n",
            "2094/2094 [==============================] - 39s 19ms/step - loss: 0.3196 - binary_accuracy: 0.8507 - val_loss: 0.7370 - val_binary_accuracy: 0.7076\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_9 (Bidirection (None, 30, 40)            6560      \n",
            "_________________________________________________________________\n",
            "bidirectional_10 (Bidirectio (None, 30, 80)            25920     \n",
            "_________________________________________________________________\n",
            "bidirectional_11 (Bidirectio (None, 40)                16160     \n",
            "_________________________________________________________________\n",
            "out (Dense)                  (None, 1)                 41        \n",
            "=================================================================\n",
            "Total params: 48,681\n",
            "Trainable params: 48,681\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z38lxMUGAI80"
      },
      "source": [
        "# Saving the model results in a file \n",
        "with open('models/model_history_pp3', 'wb') as file:\n",
        "    pkl.dump(history.history, file)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpDFAb-dAI80"
      },
      "source": [
        "## Ploting the results \n",
        "\n",
        "Here are the results for the different preprocessing options. This graph shows that the best option is to use tokenization only as the two other methods seems to worsen the accuracy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65X3wfCBAI81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "f2e9b53a-5f8e-4473-8415-b77d8e8b0a00"
      },
      "source": [
        "# Plot \n",
        "\n",
        "plot_accuracies()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-c433a470ac6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplot_accuracies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'plot_accuracies' is not defined"
          ]
        }
      ]
    }
  ]
}